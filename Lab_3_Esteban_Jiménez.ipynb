{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#0) Library Import"
      ],
      "metadata": {
        "id": "jbVH5JCwqzpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjnApjqMKk3_",
        "outputId": "0ac40f5b-4114-4ce3-fb2c-bc2f30bb97c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn-extra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFB1AvBeKeaj",
        "outputId": "26a211c4-18a3-4e77-ce11-529d63b914ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn-extra\n",
            "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.2.0)\n",
            "Installing collected packages: scikit-learn-extra\n",
            "Successfully installed scikit-learn-extra-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymanopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afj-JVv3Z_1G",
        "outputId": "c64d884b-c7f9-4b8f-eeac-47b357fe62e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymanopt\n",
            "  Downloading pymanopt-2.1.1-py3-none-any.whl (69 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m61.4/69.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from pymanopt) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from pymanopt) (1.11.2)\n",
            "Installing collected packages: pymanopt\n",
            "Successfully installed pymanopt-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir lab_1"
      ],
      "metadata": {
        "id": "RQ7RiB3IqH7L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd lab_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cQpjX6oqPfi",
        "outputId": "bb1e1906-c39f-4282-9e63-e2456d1ffe58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lab_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import cv2\n",
        "\n",
        "from sklearn.datasets import load_wine, load_iris\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn import cluster,datasets,mixture\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from numpy import linalg as LA\n",
        "from PIL import Image\n",
        "from matplotlib.image import imread\n",
        "from matplotlib.lines import Line2D\n",
        "from io import BytesIO\n",
        "from scipy.spatial import distance\n"
      ],
      "metadata": {
        "id": "UaDHG1Iuf1Pp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1)Vector embeddings.\n",
        "\n",
        "According to what we have seen in class, a vector embedding is a numeric representation of different objects(such a text, data points, etc) in a multidimentional space.\n",
        "\n",
        "Since each object can be represented as a vector, it can be incredibly useful for computing, let's say, distances between each one of them, which at the same time make possible to do a wide range of tasks, including text processing such as natural language processing (NLP).\n"
      ],
      "metadata": {
        "id": "zeDV2tcsm3RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Best distance estimation criteria\n",
        "\n",
        "Regarding the best distance criterion to estimate how far two embeddings, it all depends of the specific purpose/goals of the project and the data itself.\n",
        "\n",
        "I personally think, the euclidian distance is the most useful, since it computes the geometric distance as a line between points (vectors) in spaces of n dimensions. It is easy too understand and read, and overall, depending on the specific needs of the task, it will still be a valid criteria for doing the job.\n",
        "\n",
        "However, here is some of the criteria most commonly used, according to documentation:  \n",
        "\n",
        "1. Cosine Similarity: Cosine similarity is one of the most commonly used metrics for estimating the similarity between embeddings. It measures the cosine of the angle between two vectors and is particularly useful for measuring the similarity in direction between vectors. Cosine similarity is valuable because it is scale-invariant and focuses on the orientation of vectors rather than their magnitude.\n",
        "\n",
        "   - Why: Cosine similarity provides a measure of semantic similarity between vectors. It is often used in tasks like document retrieval, text classification, and recommendation systems to find similar documents or items.\n",
        "\n",
        "2. Euclidean Distance: The Euclidean distance measures the straight-line distance between two points in vector space. It calculates the geometric distance between embeddings in multi-dimensional space.\n",
        "\n",
        "   - Why: Euclidean distance is useful for measuring the dissimilarity or distance between embeddings. It is often employed in clustering and anomaly detection tasks to identify outliers or group similar data points together.\n",
        "\n",
        "3. Manhattan Distance: The Manhattan distance (also known as L1 distance or taxicab distance) calculates the sum of absolute differences between the coordinates of two vectors. It measures the distance along the grid-like paths in vector space.\n",
        "\n",
        "   - Why: Manhattan distance is valuable when the movement in each dimension has a cost or weight associated with it. It is used in applications like image recognition and feature engineering.\n",
        "\n",
        "4. Mahalanobis Distance: The Mahalanobis distance is a measure that takes into account the correlation between dimensions and the scales of the dimensions. It normalizes the distances based on the covariance matrix.\n",
        "\n",
        "   - Why: Mahalanobis distance is useful when the data is not spherical or when dimensions are correlated. It is used in clustering, classification, and outlier detection tasks.\n",
        "\n",
        "5. Jaccard Similarity: Jaccard similarity measures the similarity between two sets by calculating the size of their intersection divided by the size of their union. It is often used for comparing sets of items or features.\n",
        "\n",
        "   - Why: Jaccard similarity is valuable in text analysis for tasks like document similarity and recommendation systems. It is also used in natural language processing for measuring the similarity of word sets.\n",
        "\n",
        "6. Kullback-Leibler Divergence: Kullback-Leibler (KL) divergence measures the difference between two probability distributions. It quantifies how one distribution differs from another and is often used in information theory.\n",
        "\n",
        "   - Why: KL divergence is valuable for tasks involving probability distributions, such as topic modeling and language modeling. It is used to compare how similar or dissimilar two distributions are.\n",
        "\n",
        "7. Spearman Rank Correlation: Spearman's rank correlation coefficient measures the strength and direction of association between two ranked variables. It is used when the data is ordinal or when linear relationships are not appropriate.\n",
        "\n",
        "   - Why: Spearman rank correlation is valuable when assessing the relationship between rankings, such as in information retrieval or evaluating the quality of embeddings in ranking-based tasks.\n",
        "\n",
        "8. Word Embedding Evaluation: For word embeddings, specific evaluation tasks like word similarity, word analogy, or word categorization are used. These tasks involve comparing word vectors to human-annotated data to assess their semantic properties.\n",
        "\n",
        "   - Why: Word embedding evaluation tasks are tailored to assess how well embeddings capture semantic relationships between words, making them important for assessing the quality of word vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kvc2tV0xm51s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Text selection"
      ],
      "metadata": {
        "id": "KgtM_Bm5nAF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##a.The chosen text is a brief explanation of the Golden Age arc from Berserk manga."
      ],
      "metadata": {
        "id": "ZS2d6rV_obxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##b. The text will be divide in 20 paragraphs and stores in a list."
      ],
      "metadata": {
        "id": "-Rgq7HYCo91z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "berserk_text = [\n",
        "    \"'Berserk: The Golden Age Arc' is a series of three animated films that adapt Kentaro Miura's dark fantasy manga, 'Berserk.' The story is set in a brutal and unforgiving medieval world, where violence and despair reign supreme. The narrative primarily follows the journey of Guts, a lone mercenary with a tragic past.\",\n",
        "    \"The story begins with a dramatic and intense introduction to Guts, who is known as the 'Black Swordsman.' He's a formidable warrior, and his path crosses with the Band of the Hawk, a renowned mercenary group led by the charismatic and ambitious Griffith.\",\n",
        "    \"Griffith, the leader of the Band of the Hawk, is a captivating and enigmatic figure. He harbors an ambitious dream of obtaining his own kingdom, a vision that he pursues relentlessly. Griffith recognizes Guts' extraordinary combat abilities and quickly recruits him into the Hawks, viewing him as an indispensable asset.\",\n",
        "    \"As Guts becomes a part of the Band of the Hawk, the story delves into the development of his relationship with Griffith. Their camaraderie deepens, and Guts becomes not only Griffith's most trusted soldier but also his close confidant.\",\n",
        "    \"Guts' arrival in the Band of the Hawk has far-reaching consequences, particularly in the realm of relationships. Casca, the capable and determined female commander of the Hawks, develops complicated feelings for Guts, setting the stage for an intricate love triangle.\",\n",
        "    \"The complexities of Guts and Griffith's friendship come to the forefront as Guts seeks his own path and independence within the Hawks. His desire to define himself as more than just a sword for Griffith strains their relationship.\",\n",
        "    \"Griffith, in turn, grapples with his own emotions, including admiration for Guts' prowess and the jealousy that arises as Guts continues to ascend within the ranks of the Hawks.\",\n",
        "    \"Griffith's relentless pursuit of his dream leads him down a dark and fateful path. He makes a pact with supernatural entities known as the God Hand, sealing a fate that will have catastrophic consequences for himself and the Band of the Hawk.\",\n",
        "    \"The Eclipse, the culmination of Griffith's dark pact, unfolds in horrifying detail. Griffith is imprisoned and subjected to unspeakable torture, while the Band of the Hawk faces a nightmarish ordeal that leads to their disbandment.\",\n",
        "    \"Guts, bearing the emotional scars of the Eclipse and a burning desire for revenge, embarks on a solitary quest. He becomes the 'Black Swordsman,' wielding a massive sword known as the Dragonslayer and battling grotesque demons in a world consumed by darkness.\",\n",
        "    \"As Guts navigates this grim and perilous world, he encounters a diverse cast of characters, each with their own complex backgrounds and motivations. These include Puck, a mischievous elf; Isidro, a young rogue; Serpico, a cunning swordsman; and Farnese, a noblewoman turned inquisitor.\",\n",
        "    \"Meanwhile, Casca's mental and emotional well-being becomes a focal point of the narrative. Her traumatic experience during the Eclipse has left her in a fragile and vulnerable state. Guts takes on the monumental task of protecting and caring for her as they confront the horrors of their world together.\",\n",
        "    \"The story also explores the themes of fate, free will, sacrifice, and the human condition. In a world overrun by darkness and malevolence, characters are forced to make agonizing choices that shape their destinies.\",\n",
        "    \"Guts' relentless pursuit of vengeance leads him on a collision course with Griffith, now reborn as Femto, one of the God Hand's most powerful members. Their confrontations are intense and emotionally charged, reflecting the depth of their connection and betrayal.\",\n",
        "    \"Throughout the narrative, the artwork and storytelling by Kentaro Miura are renowned for their intricate detail and profound symbolism. The world of 'Berserk' is visually striking and emotionally evocative.\",\n",
        "    \"Guts acquires a new weapon, the Dragonslayer, a colossal sword capable of slaying supernatural beings. This weapon becomes emblematic of his determination to combat the demonic forces plaguing the world.\",\n",
        "    \"The group faces numerous Apostles, demonic entities that serve the God Hand, and other supernatural threats as they journey together, their dynamics evolving as they confront their own inner demons.\",\n",
        "    \"The narrative remains a blend of brutal action, complex characters, and a bleak, morally ambiguous world. 'Berserk' is celebrated for its mature themes and exploration of the darker aspects of humanity.\",\n",
        "    \"The story ultimately serves as a compelling introduction to the larger 'Berserk' saga, leaving viewers and readers eager to explore the manga's extensive and ongoing narrative, which continues to captivate audiences worldwide.\",\n",
        "    \"'Berserk: The Golden Age Arc' stands as a testament to Kentaro Miura's storytelling prowess and his ability to craft a narrative that is simultaneously brutal, thought-provoking, and emotionally resonant. It remains a cornerstone of the dark fantasy genre and continues to influence creators and fans alike.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "6ezXcfDnZWPC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##c. Embedding tool slection\n",
        "For this particular exercise, we'll be using the \"roberta-base-squad2\" model imported from the web site huggingface.com.\n",
        "\n",
        "As a RoBERTA model (transformer-based models), it uses a specific approach for generating word embeddings, which forms the foundation for its natural language understanding capabilities.  It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.\n",
        "\n",
        "Here's a brief overview of the embedding generation logic and approach used by RoBERTa:\n",
        "\n",
        "1. Tokenization: The first step in generating embeddings is to tokenize the input text. Tokenization breaks down the input text into smaller units, typically words or subwords. These units are called tokens.\n",
        "\n",
        "2. Word Embeddings: For each token, RoBERTa uses an embedding layer to convert it into a fixed-size vector representation called a word embedding. These word embeddings capture the meaning and context of each token. RoBERTa uses Byte Pair Encoding (BPE) subword tokenization, which means it can represent both words and subwords effectively.\n",
        "\n",
        "3. Positional Encodings: To account for the order of tokens in a sequence, RoBERTa incorporates positional encodings. These encodings are added to the word embeddings to provide information about the position of each token in the input sequence. This allows RoBERTa to understand the sequential structure of the text.\n",
        "\n",
        "4. Self-Attention Mechanism: One of the key components of the transformer architecture, which RoBERTa is based on, is the self-attention mechanism. It allows the model to weigh the importance of different tokens in relation to each other, considering both their content (word embeddings) and position (positional encodings). This self-attention mechanism enables RoBERTa to capture dependencies and relationships between words across the entire input sequence.\n",
        "\n",
        "5. Transformer Layers: RoBERTa consists of multiple transformer layers stacked on top of each other. Each transformer layer refines the representations of tokens through self-attention and feedforward neural networks. This hierarchical structure enables the model to capture increasingly complex patterns and dependencies in the text.\n",
        "\n",
        "6. Layer Aggregation: After processing the input through multiple transformer layers, RoBERTa typically aggregates the output embeddings from all layers to obtain a final representation of the input text. This aggregation can be done in various ways, such as taking the mean or concatenating the embeddings.\n",
        "\n",
        "7. Contextualized Embeddings: The embeddings generated by RoBERTa are contextualized, meaning they take into account the entire context of the input text. This allows the model to understand the meaning of a word or token in the context of the surrounding words.\n",
        "\n",
        "8. Downstream Tasks: Once the embeddings are generated, RoBERTa can be fine-tuned for specific downstream natural language processing tasks, such as text classification, question answering, named entity recognition, and more. During fine-tuning, additional task-specific layers are added on top of the RoBERTa model to adapt it to the specific task.\n",
        "\n",
        "In other words, RoBERTa generates word embeddings by tokenizing the input text, applying word embeddings and positional encodings, leveraging self-attention mechanisms, and using a stack of transformer layers. These embeddings capture rich contextual information, making RoBERTa highly effective for a wide range of natural language understanding tasks.\n"
      ],
      "metadata": {
        "id": "zHhT0FEKpVls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##d. Model run and results"
      ],
      "metadata": {
        "id": "erjg2Zbc2gib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "questions = [\n",
        "    'Who is Guts?',\n",
        "    'Who is Griffith?',\n",
        "    'Who is Casca?',\n",
        "    'Why did Guts leave the Band of the Hawk?',\n",
        "    'What happened during the eclipse?'\n",
        "]\n",
        "\n",
        "answers_list = []  # Create a list to store answers for each question\n",
        "scores_list = []   # Create a list to store scores for each question\n",
        "index_list = []   # Create a list to store paragraph index for each question\n",
        "\n",
        "# Load model & tokenizer outside of the loop since it's not necessary to load them repeatedly\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Iterate through each question\n",
        "for question in questions:\n",
        "\n",
        "    paragraph_answers = []  # Create a list to store answers for each paragraph\n",
        "    paragraph_scores = []   # Create a list to store scores for each paragraph\n",
        "    paragraph_index = []  # Create a list to store index for each paragraph\n",
        "\n",
        "    for i, paragraph in enumerate(berserk_text):\n",
        "        QA_input = {'question': question, 'context': paragraph}\n",
        "        res = nlp(QA_input)\n",
        "        paragraph_answers.append(res['answer'])\n",
        "        paragraph_scores.append(res['score'])\n",
        "        paragraph_index.append(i)\n",
        "\n",
        "    # Sort answers and scores for this question in descending order of score\n",
        "    sorted_data = sorted(zip(paragraph_scores, paragraph_answers, paragraph_index), reverse=True)\n",
        "    sorted_answers = [ans for _, ans, _ in sorted_data]\n",
        "    sorted_scores = [score for score, _, _ in sorted_data]\n",
        "    sorted_paragraph_indices = [idx for _, _, idx in sorted_data]\n",
        "\n",
        "    answers_list.append(sorted_answers)  # Append sorted answers to the list\n",
        "    scores_list.append(sorted_scores)   # Append sorted scores to the list\n",
        "    index_list.append(sorted_paragraph_indices) # Append sorted index to the list\n",
        "\n",
        "# Print the answers, scores, and corresponding paragraphs for each question\n",
        "for i, question in enumerate(questions):\n",
        "    print('Question:', question)\n",
        "    for j, answer in enumerate(answers_list[i]):\n",
        "        paragraph_index = index_list[i][j]\n",
        "        paragraph = berserk_text[paragraph_index]\n",
        "        print(f\"From paragraph {paragraph_index + 1} - Answer {j + 1}: {answer} (Score: {scores_list[i][j]})\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LlXu9sPNKKr",
        "outputId": "c53ecf0f-24b9-4667-c676-70057b9c2bd5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who is Guts?\n",
            "From paragraph 10 - Answer 1: Black Swordsman (Score: 0.5023044347763062)\n",
            "From paragraph 1 - Answer 2: a lone mercenary (Score: 0.496697336435318)\n",
            "From paragraph 2 - Answer 3: Black Swordsman (Score: 0.32001352310180664)\n",
            "From paragraph 4 - Answer 4: close confidant (Score: 0.23824971914291382)\n",
            "From paragraph 7 - Answer 5: prowess (Score: 0.1918916255235672)\n",
            "From paragraph 11 - Answer 6: mischievous elf (Score: 0.17804814875125885)\n",
            "From paragraph 5 - Answer 7: commander of the Hawks (Score: 0.05160028114914894)\n",
            "From paragraph 14 - Answer 8: Femto (Score: 0.04900278523564339)\n",
            "From paragraph 3 - Answer 9: indispensable asset (Score: 0.040582384914159775)\n",
            "From paragraph 12 - Answer 10: Casca (Score: 0.023408317938447)\n",
            "From paragraph 6 - Answer 11: a sword for Griffith (Score: 0.017718574032187462)\n",
            "From paragraph 16 - Answer 12: supernatural beings (Score: 0.010246380232274532)\n",
            "From paragraph 9 - Answer 13: Band of the Hawk (Score: 0.003755023004487157)\n",
            "From paragraph 8 - Answer 14: God Hand (Score: 0.0026061690878123045)\n",
            "From paragraph 19 - Answer 15: Berserk (Score: 0.00030244243680499494)\n",
            "From paragraph 17 - Answer 16: God Hand (Score: 8.162459562299773e-05)\n",
            "From paragraph 15 - Answer 17: Berserk (Score: 1.6930287529248744e-05)\n",
            "From paragraph 18 - Answer 18: complex characters (Score: 5.5986120059969835e-06)\n",
            "From paragraph 13 - Answer 19: malevolence (Score: 1.23528923268168e-06)\n",
            "From paragraph 20 - Answer 20: Kentaro Miura (Score: 1.081188997886784e-06)\n",
            "\n",
            "Question: Who is Griffith?\n",
            "From paragraph 3 - Answer 1: the leader of the Band of the Hawk (Score: 0.5392065644264221)\n",
            "From paragraph 7 - Answer 2: Guts (Score: 0.3820396959781647)\n",
            "From paragraph 2 - Answer 3: charismatic and ambitious (Score: 0.3545457124710083)\n",
            "From paragraph 14 - Answer 4: one of the God Hand's most powerful members (Score: 0.3418923318386078)\n",
            "From paragraph 4 - Answer 5: most trusted soldier (Score: 0.3276318311691284)\n",
            "From paragraph 10 - Answer 6: Black Swordsman (Score: 0.11090210825204849)\n",
            "From paragraph 8 - Answer 7: God Hand (Score: 0.03557099029421806)\n",
            "From paragraph 9 - Answer 8: imprisoned (Score: 0.030599327757954597)\n",
            "From paragraph 6 - Answer 9: a sword (Score: 0.029283432289958)\n",
            "From paragraph 11 - Answer 10: noblewoman turned inquisitor (Score: 0.0002112636575475335)\n",
            "From paragraph 12 - Answer 11: Guts (Score: 8.498496754327789e-05)\n",
            "From paragraph 5 - Answer 12: commander of the Hawks (Score: 7.779199950164184e-05)\n",
            "From paragraph 17 - Answer 13: God Hand (Score: 7.569467561552301e-05)\n",
            "From paragraph 1 - Answer 14: Guts, a lone mercenary (Score: 1.0358957297285087e-05)\n",
            "From paragraph 16 - Answer 15: Guts (Score: 4.977673597750254e-06)\n",
            "From paragraph 13 - Answer 16: characters are forced to make agonizing choices that shape their destinies. (Score: 2.1202588413871126e-06)\n",
            "From paragraph 18 - Answer 17: complex characters, and a bleak, morally ambiguous world. (Score: 9.315063493886555e-07)\n",
            "From paragraph 20 - Answer 18: Kentaro Miura (Score: 3.4335243981331587e-07)\n",
            "From paragraph 15 - Answer 19: Kentaro Miura are renowned for their intricate detail and profound symbolism. (Score: 2.8092375714550144e-07)\n",
            "From paragraph 19 - Answer 20: Berserk (Score: 1.5947892961776233e-07)\n",
            "\n",
            "Question: Who is Casca?\n",
            "From paragraph 5 - Answer 1: the capable and determined female commander of the Hawks (Score: 0.3024120330810547)\n",
            "From paragraph 12 - Answer 2: Guts (Score: 0.08448158949613571)\n",
            "From paragraph 3 - Answer 3: Guts (Score: 0.00012862177391070873)\n",
            "From paragraph 10 - Answer 4: Black Swordsman (Score: 5.300088014337234e-05)\n",
            "From paragraph 1 - Answer 5: Guts, a lone mercenary (Score: 5.052648702985607e-05)\n",
            "From paragraph 11 - Answer 6: Farnese, a noblewoman turned inquisitor (Score: 3.810703128692694e-05)\n",
            "From paragraph 17 - Answer 7: God Hand (Score: 2.3321243133977987e-05)\n",
            "From paragraph 8 - Answer 8: God Hand (Score: 7.472489414794836e-06)\n",
            "From paragraph 7 - Answer 9: Guts (Score: 6.889783890073886e-06)\n",
            "From paragraph 9 - Answer 10: Band of the Hawk (Score: 6.171142103994498e-06)\n",
            "From paragraph 6 - Answer 11: Guts (Score: 3.9769506656739395e-06)\n",
            "From paragraph 4 - Answer 12: Griffith (Score: 2.7797821076092077e-06)\n",
            "From paragraph 14 - Answer 13: Femto (Score: 2.5770036700123455e-06)\n",
            "From paragraph 13 - Answer 14: characters are forced to make agonizing choices that shape their destinies. (Score: 6.809901833548793e-07)\n",
            "From paragraph 2 - Answer 15: Guts (Score: 6.724209242747747e-07)\n",
            "From paragraph 20 - Answer 16: Kentaro Miura (Score: 3.4185438835265813e-07)\n",
            "From paragraph 15 - Answer 17: Kentaro Miura (Score: 1.6132675284552533e-07)\n",
            "From paragraph 18 - Answer 18: complex characters, and a bleak, morally ambiguous world. (Score: 1.0290903418308517e-07)\n",
            "From paragraph 19 - Answer 19: manga (Score: 4.2463636162892726e-08)\n",
            "From paragraph 16 - Answer 20: Dragonslayer, a colossal sword capable of slaying supernatural beings. (Score: 1.4491325472931749e-08)\n",
            "\n",
            "Question: Why did Guts leave the Band of the Hawk?\n",
            "From paragraph 6 - Answer 1: His desire to define himself as more than just a sword for Griffith (Score: 0.14469896256923676)\n",
            "From paragraph 14 - Answer 2: relentless pursuit of vengeance (Score: 0.11541558057069778)\n",
            "From paragraph 9 - Answer 3: nightmarish ordeal (Score: 0.03183808922767639)\n",
            "From paragraph 10 - Answer 4: burning desire for revenge (Score: 0.0064464653842151165)\n",
            "From paragraph 1 - Answer 5: a tragic past (Score: 0.0005185448098927736)\n",
            "From paragraph 17 - Answer 6: confront their own inner demons (Score: 0.0005127978511154652)\n",
            "From paragraph 8 - Answer 7: relentless pursuit of his dream (Score: 0.0004781072202604264)\n",
            "From paragraph 7 - Answer 8: jealousy (Score: 0.0004211303312331438)\n",
            "From paragraph 11 - Answer 9: grim and perilous world (Score: 0.0003314822097308934)\n",
            "From paragraph 3 - Answer 10: ambitious dream of obtaining his own kingdom (Score: 7.540761998825474e-06)\n",
            "From paragraph 16 - Answer 11: determination to combat the demonic forces plaguing the world (Score: 6.403840870916611e-06)\n",
            "From paragraph 2 - Answer 12: warrior (Score: 2.6770469503389904e-06)\n",
            "From paragraph 13 - Answer 13: agonizing choices (Score: 2.001654593186686e-06)\n",
            "From paragraph 19 - Answer 14: a compelling introduction to the larger 'Berserk' saga (Score: 1.6879159829841228e-06)\n",
            "From paragraph 4 - Answer 15: development of his relationship with Griffith (Score: 7.49005380384915e-07)\n",
            "From paragraph 5 - Answer 16: love triangle (Score: 6.70169015393185e-07)\n",
            "From paragraph 18 - Answer 17: bleak, morally ambiguous world (Score: 2.567376782280917e-07)\n",
            "From paragraph 12 - Answer 18: traumatic experience during the Eclipse (Score: 1.2945265837061015e-07)\n",
            "From paragraph 20 - Answer 19: Berserk: The Golden Age Arc (Score: 3.308872464913293e-08)\n",
            "From paragraph 15 - Answer 20: Berserk' is visually striking and emotionally evocative. (Score: 2.5560268568369793e-08)\n",
            "\n",
            "Question: What happened during the eclipse?\n",
            "From paragraph 12 - Answer 1: traumatic experience (Score: 0.11664579063653946)\n",
            "From paragraph 9 - Answer 2: Griffith is imprisoned and subjected to unspeakable torture (Score: 0.07233041524887085)\n",
            "From paragraph 10 - Answer 3: emotional scars (Score: 0.005048003979027271)\n",
            "From paragraph 7 - Answer 4: jealousy (Score: 0.00019466281810309738)\n",
            "From paragraph 16 - Answer 5: slaying supernatural beings (Score: 0.0001298512943321839)\n",
            "From paragraph 8 - Answer 6: makes a pact with supernatural entities (Score: 1.537541720608715e-05)\n",
            "From paragraph 5 - Answer 7: love triangle (Score: 1.027731104841223e-05)\n",
            "From paragraph 17 - Answer 8: demonic entities that serve the God Hand, and other supernatural threats (Score: 6.4031723923108075e-06)\n",
            "From paragraph 6 - Answer 9: friendship (Score: 5.598597454081755e-06)\n",
            "From paragraph 13 - Answer 10: agonizing choices that shape their destinies. (Score: 4.507953690335853e-06)\n",
            "From paragraph 14 - Answer 11: vengeance (Score: 4.193491349724354e-06)\n",
            "From paragraph 4 - Answer 12: Griffith. (Score: 2.680980287550483e-06)\n",
            "From paragraph 15 - Answer 13: intricate detail and profound symbolism. (Score: 1.1755994364648359e-06)\n",
            "From paragraph 2 - Answer 14: dramatic and intense introduction to Guts (Score: 3.9935937934387766e-07)\n",
            "From paragraph 18 - Answer 15: brutal action, complex characters, and a bleak, morally ambiguous world. (Score: 3.0416387630793906e-07)\n",
            "From paragraph 19 - Answer 16: Berserk (Score: 2.652743091857701e-07)\n",
            "From paragraph 11 - Answer 17: inquisitor. (Score: 2.1582997078439803e-07)\n",
            "From paragraph 20 - Answer 18: Berserk: The Golden Age Arc (Score: 1.8849432592560333e-07)\n",
            "From paragraph 3 - Answer 19: Guts (Score: 1.3883227722999436e-07)\n",
            "From paragraph 1 - Answer 20: Guts, a lone mercenary with a tragic past. (Score: 3.821483929300484e-08)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answers obtained for each question seems to be dependant of how much context can the input piece of text provide. The less related the question is to the paragraph, the lower the score obstained. This make sense, since we cannot expect to obtain a logic answer from the model if it hasn't been trained with the \"context\" data, just like the nautral lenguage dynamics.\n",
        "\n"
      ],
      "metadata": {
        "id": "o3Ups1fg2oTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Model improvement\n",
        "\n",
        "Currently, the improving of the robustness of models such as the used above is matter of research. However, there are speficic areas of research where advancements could be made (one of them is Reinforcemente feedback, which is one of the topic seen in this class):\n",
        "\n",
        "\n",
        "1. Handling Ambiguity: Natural language is inherently ambiguous, and improving a system's ability to handle various types of ambiguity is crucial. This includes handling polysemy (multiple meanings of a word), homonymy (words with the same spelling but different meanings), and syntactic ambiguity. Developing models that can disambiguate effectively and select the most contextually appropriate interpretation is a challenge.\n",
        "\n",
        "2. Commonsense Reasoning: Enhancing models' ability to reason about common sense and world knowledge is essential for better understanding context. Integrating external knowledge sources, like structured databases or commonsense knowledge graphs, into models can help them make more informed predictions and avoid absurd or incorrect answers.\n",
        "\n",
        "3. Multi-turn Conversations: Expanding models to handle multi-turn conversations and maintaining context across multiple interactions is critical for building more functional chatbots and dialogue systems. Ensuring that models can keep track of previous conversation history and provide coherent responses is an ongoing area of research.\n",
        "\n",
        "4. Domain Adaptation: Enabling models to adapt to specific domains or industries is important for practical applications. Developing techniques for efficient domain adaptation and fine-tuning to make models more domain-aware and better at handling specialized terminology and context is essential.\n",
        "\n",
        "5. Out-of-Distribution Data: Models should be more robust to out-of-distribution data—data that differs significantly from the training data. Handling unexpected inputs gracefully and providing sensible responses even in unanticipated scenarios is a challenge.\n",
        "\n",
        "6. Bias and Fairness: Addressing bias and fairness issues is crucial to ensure that these systems do not reinforce harmful stereotypes or exhibit biased behavior. Ongoing research focuses on developing debiasing techniques and fairness-aware training.\n",
        "\n",
        "7. Explainability and Interpretability: Enhancing the explainability and interpretability of models is important for building trust and facilitating human-machine collaboration. Research in this area aims to make models more transparent and capable of providing explanations for their predictions.\n",
        "\n",
        "8. Multilingual and Cross-lingual Understanding: Extending models to support multiple languages and cross-lingual understanding is valuable for global applications. Developing models that can transfer knowledge across languages and adapt to low-resource languages is an active area of research.\n",
        "\n",
        "9. Zero-shot and Few-shot Learning: Enabling models to generalize and provide reasonable answers even when faced with questions or tasks they haven't been explicitly trained on is a challenging goal. Zero-shot and few-shot learning techniques aim to improve this capability.\n",
        "\n",
        "10. User Feedback and Reinforcement Learning: Integrating user feedback and reinforcement learning into the training process can help fine-tune models based on real-world interactions, making them more user-centric and adaptive.\n",
        "\n",
        "11. Robustness to Adversarial Inputs: Models should be made more robust to adversarial inputs, such as intentionally crafted inputs designed to mislead the system. Adversarial training and defenses against such attacks are areas of active research.\n",
        "\n",
        "12. Ethical Considerations: Addressing ethical considerations in natural language understanding systems, including issues related to privacy, consent, and responsible AI, is an essential part of robust system development.\n",
        "\n"
      ],
      "metadata": {
        "id": "v4ZjMoCf6NFr"
      }
    }
  ]
}